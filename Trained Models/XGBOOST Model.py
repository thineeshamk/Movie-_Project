# -*- coding: utf-8 -*-
"""Untitled82.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lOfFQQ7EcpNvXuNYCIlrjqSEl7T-T5cl
"""

import warnings
warnings.filterwarnings("ignore")

# 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.decomposition import TruncatedSVD
from scipy.stats import uniform, loguniform
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegressionCV

# 2. Load Datasets
meta_df = pd.read_excel("Final Dataset.xlsx")
bert_df = pd.read_excel("longformer_embeddings.xlsx")

bert_df = bert_df.drop(columns=["Plot Synopsis", "Title"], errors="ignore")

print("Metadata shape:", meta_df.shape)
print("BERT shape (before SVD):", bert_df.shape)

# 3. Preprocessing and Feature Engineering
categorical_vars = ["MPA", "1st Genre"]

meta_df["MPA"] = meta_df["MPA"].apply(lambda x: x if x in ["PG-13", "R"] else "Other")

main_genres = ["Action", "Drama", "Comedy", "Biography"]
meta_df["1st Genre"] = meta_df["1st Genre"].apply(lambda x: x if x in main_genres else "Other")

encoder = OneHotEncoder(drop="first", sparse_output=False, handle_unknown='ignore')
encoded_cats = encoder.fit_transform(meta_df[categorical_vars])
encoded_cat_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out(categorical_vars))

numerical_vars = ["budget", "Duration_Minutes", "First Actor Avg", "Second Actor Avg", "Average IMDb Rating"]
numerical_df = meta_df[numerical_vars].reset_index(drop=True)

# 4. Dimensionality Reduction (SVD)
scaler = StandardScaler()
bert_scaled = scaler.fit_transform(bert_df.values)

max_components = min(300, bert_scaled.shape[1])
svd_analysis = TruncatedSVD(n_components=max_components, random_state=42)
svd_analysis.fit(bert_scaled)

cumulative_variance = np.cumsum(svd_analysis.explained_variance_ratio_)
n_components_80_var = int(np.argmax(cumulative_variance >= 0.80) + 1)

print("Components for 80% variance:", n_components_80_var)

svd = TruncatedSVD(n_components=n_components_80_var, random_state=42)
bert_svd = svd.fit_transform(bert_scaled)
bert_svd_df = pd.DataFrame(bert_svd, columns=[f"bert_svd_{i}" for i in range(bert_svd.shape[1])])

# Combine features
X = pd.concat([
    numerical_df.reset_index(drop=True),
    encoded_cat_df.reset_index(drop=True),
    bert_svd_df.reset_index(drop=True)
], axis=1)

# Target variable
y = LabelEncoder().fit_transform(
    meta_df["Rating"].apply(lambda r: "Success" if r >= 6.5 else "Unsuccess")
)

# 5. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 6. Define Stacking Ensemble
base_xgb = XGBClassifier(
    random_state=42,
    n_jobs=-1,
    use_label_encoder=False,
    eval_metric="logloss"
)

base_lgbm = LGBMClassifier(
    random_state=42,
    n_jobs=-1
)

estimators = [
    ("xgb", base_xgb),
    ("lgbm", base_lgbm)
]

meta_model = LogisticRegressionCV(
    cv=5,
    random_state=42,
    max_iter=1000,
    Cs=10
)

stacking_classifier = StackingClassifier(
    estimators=estimators,
    final_estimator=meta_model,
    cv=5,
    n_jobs=-1,
    passthrough=False
)

# 7. Hyperparameter Tuning
param_distributions = {
    "xgb__n_estimators": [50, 100, 200, 400],
    "xgb__learning_rate": loguniform(0.01, 0.3),
    "xgb__max_depth": [3, 4, 6],
    "xgb__gamma": loguniform(0.01, 1.0),
    "xgb__reg_alpha": loguniform(0.01, 10.0),
    "xgb__reg_lambda": loguniform(0.01, 10.0),
    "xgb__subsample": uniform(0.6, 0.4),
    "xgb__colsample_bytree": uniform(0.6, 0.4),

    "lgbm__n_estimators": [50, 100, 200, 400],
    "lgbm__learning_rate": loguniform(0.01, 0.3),
    "lgbm__num_leaves": [31, 50, 100],
    "lgbm__min_child_samples": [10, 20, 30],
    "lgbm__reg_alpha": loguniform(0.01, 10.0),
    "lgbm__reg_lambda": loguniform(0.01, 10.0),
    "lgbm__feature_fraction": uniform(0.6, 0.4),
    "lgbm__bagging_fraction": uniform(0.6, 0.4),
    "lgbm__bagging_freq": [1, 5, 10]
}

random_search = RandomizedSearchCV(
    estimator=stacking_classifier,
    param_distributions=param_distributions,
    n_iter=50,
    cv=3,
    scoring="f1_weighted",
    n_jobs=-1,
    random_state=42,
    verbose=2
)

print("Starting hyperparameter search...")
random_search.fit(X_train, y_train)
print("Hyperparameter search complete.")

print("Best Parameters Found:")
print(random_search.best_params_)

best_model = random_search.best_estimator_